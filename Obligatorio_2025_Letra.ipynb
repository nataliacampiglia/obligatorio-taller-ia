{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obligatorio - Taller Agentes Inteligentes 2025\n",
    "\n",
    "En este trabajo obligatorio aplicaremos los conceptos vistos en el curso para dise√±ar, implementar y evaluar agentes capaces de aprender a jugar al cl√°sico **Breakout** de Atari, utilizando el entorno provisto por Farama Gymnasium ([https://ale.farama.org/environments/breakout/](https://ale.farama.org/environments/breakout/)). \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/oMxHgRrISJsAAAAM/atari-deep-learning.gif\" alt=\"Atari Deep Learning\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "El ejercicio se enmarca en un contexto de aprendizaje pr√°ctico, donde trabajaremos con las interfaces est√°ndar de Gymnasium para:\n",
    "\n",
    "1. **Profundizar en algoritmos de valor**: implementaremos y compararemos dos variantes de Q-Learning basadas en redes neuronales profundas:\n",
    "   * **Deep Q Learning (DQN)**\n",
    "   * **Double Deep Q Learning (DDQN)**\n",
    "2. **Evaluar rendimiento y estabilidad**: registraremos las recompensas obtenidas durante el entrenamiento de cada agente y analizaremos su comportamiento mediante gr√°ficas comparativas.\n",
    "3. **Demostrar resultados de forma visual**: capturaremos v√≠deos que muestren a cada agente ‚Äúresolviendo‚Äù el entorno, entendido como la habilidad de romper al menos cinco bloques en una partida.\n",
    "\n",
    "Debido a las limitaciones de tiempo y c√≥mputo propias de un entorno de curso, no se espera entrenar modelos durante m√°s de diez horas por agente. Por ello, ser√° fundamental:\n",
    "\n",
    "* Integrar puntos de **checkpoint** para guardar peri√≥dicamente los pesos de la red.\n",
    "* Seguir en los puntos 2 y 3 la arquitectura y t√©cnicas originales propuestas en los papers seminales de DQN y DDQN, dejando la experimentaci√≥n adicional para el punto extra.\n",
    "* Flexibilizar la notebook de gu√≠a: pueden reorganizarla o dividirla en m√∫ltiples archivos seg√∫n su conveniencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos y tareas\n",
    "\n",
    "1. **Completar la implementaci√≥n**\n",
    "   * Rellenar el c√≥digo faltante en la notebook y en los m√≥dulos auxiliares para que los agentes puedan interactuar con el entorno de Breakout.\n",
    "2. **Entrenar agentes**\n",
    "   * Ajustar y entrenar un **DQN** que alcance la condici√≥n de ‚Äúresolver‚Äù (romper ‚â• 10 bloques).\n",
    "   * Ajustar y entrenar un **DDQN** con la misma meta de desempe√±o.\n",
    "3. **Visualizar y analizar resultados**\n",
    "   * Generar **gr√°ficas comparativas** de las recompensas obtenidas por ambos agentes en el mismo entorno (una gr√°fica por ambiente). Adem√°s se sugiere gr√°ficas que muestren el valor de la funci√≥n de valor Q para cada agente.\n",
    "   * Extraer **al menos dos conclusiones** por gr√°fica, comentando diferencias en convergencia, estabilidad y comportamiento exploratorio.\n",
    "4. **Registro de demostraciones**\n",
    "   * Grabar y entregar un **video demostrativo** de cada agente resolviendo el entorno.\n",
    "5. **Experimentaci√≥n**\n",
    "   * Probar otras arquitecturas, t√©cnicas de mejora o m√≥dulos de procesamiento de entradas m√°s avanzados, documentando brevemente su impacto/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterios de evaluaci√≥n\n",
    "\n",
    "| Criterio                                  | Descripci√≥n                                                  | Peso |\n",
    "| ----------------------------------------- | ------------------------------------------------------------ | ---- |\n",
    "| **Implementaci√≥n y rendimiento**          | DQN y DDQN completados; cada agente rompe ‚â• 10 bloques       | 40%  |\n",
    "| **Estructura y narrativa de la notebook** | Secciones claras, explicaci√≥n de decisiones, ‚Äúhistoria‚Äù      | 20%  |\n",
    "| **An√°lisis de resultados**                | Gr√°ficas comparativas; ‚â• 2 conclusiones por gr√°fico          | 20%  |\n",
    "| **Presentaci√≥n visual**                   | V√≠deos demostrativos de cada agente                          | 10%  |\n",
    "| **Experimentaci√≥n**                       | Experimentaci√≥n adicional documentada y analizada brevemente | 10%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliograf√≠a\n",
    "\n",
    "* **Mnih, V.**, Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ‚Ä¶ Hassabis, D. (2013). *Playing Atari with Deep Reinforcement Learning*. [arXiv:1312.5602](https://arxiv.org/abs/1312.5602)\n",
    "* **van Hasselt, H.**, Guez, A., & Silver, D. (2015). *Deep Reinforcement Learning with Double Q-learning*. [arXiv:1509.06461](https://arxiv.org/abs/1509.06461)\n",
    "* **Sutton, R. S.**, & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.), cap√≠tulo 16.5: ‚ÄúHuman-level Video Game Play‚Äù. MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc6t9etEt9I2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwHCw6PMt9I3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import gymnasium\n",
    "import ale_py\n",
    "from utils import make_env, show_observation_stack\n",
    "from IPython.display import Video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALE (Atari Learning Environment) es un entorno de aprendizaje para videojuegos de Atari. En este caso, utilizaremos el entorno de Breakout. Es necesario entender que se separa el entorno de los roms de Atari, que son los juegos en s√≠. El entorno de ALE permite interactuar con los juegos de Atari a trav√©s de una API est√°ndar, facilitando la implementaci√≥n de algoritmos de aprendizaje por refuerzo.\n",
    "\n",
    "Debemos instalar los roms por separado, para ello primero tenemos que saber donde est√°n los roms de Atari. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gymnasium.register_envs(ale_py) # registramos todos los entornos de ale_py\n",
    "ruta_init = ale_py.roms.__file__ # debemos saber donde se encuentra la carpeta roms\n",
    "ALE_ROMS_PATH = os.path.dirname(ruta_init)\n",
    "print(ALE_ROMS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar los siguientes comando para instalar los roms y colocalos en la carpeta correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"autorom[accept-rom-license]\"\n",
    "# !AutoROM --accept-license --install-dir {ALE_ROMS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijamos la semilla para que los resultados sean reproducibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic=True # https://discuss.pytorch.org/t/what-is-the-differenc-between-cudnn-deterministic-and-cudnn-benchmark/38054\n",
    "torch.backends.cudnn.benchmark=True # https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/4\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que dispositivo tenemos, si es GPU, MPS o CPU. **El uso de GPU es altamente recomendable** para acelerar el entrenamiento de los modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas constantes\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesando Atari\n",
    "\n",
    "Para reproducir fielmente el enfoque de Mnih et al. (2013) y reducir la carga computacional al trabajar con im√°genes de Atari (210 √ó 160 p√≠xeles, 128 colores), aplicamos el siguiente preprocesado œÜ a las √∫ltimas **4** frames del entorno:\n",
    "\n",
    "1. **Conversi√≥n a escala de grises**\n",
    "   Eliminamos la informaci√≥n de color (GRAYSCALE = True), pues la luminosidad es suficiente para capturar la din√°mica de juego y reduce dr√°sticamente la dimensionalidad de la entrada.\n",
    "\n",
    "2. **Down-sampling y recorte**\n",
    "   * Redimensionamos la imagen original a 110 √ó 84 p√≠xeles, manteniendo la proporci√≥n horizontal.\n",
    "   * Recortamos un √°rea central de 84 √ó 84 p√≠xeles que contiene la ‚Äúzona de juego‚Äù, descartando bordes innecesarios.\n",
    "     Este paso (SCREEN_SIZE = 84) no solo concentra la atenci√≥n del modelo en la regi√≥n relevante, sino que tambi√©n garantiza un tama√±o cuadrado compatible con las implementaciones de convoluciones en GPU.\n",
    "\n",
    "3. **Saltos temporales (frame skipping)**\n",
    "   Procesamos cada 4 frames (SKIP_FRAMES = 4), repitiendo la misma acci√≥n durante esos pasos. Esto reduce la redundancia temporal, acelera el entrenamiento y mantiene la coherencia del movimiento de la paleta y la bola.\n",
    "\n",
    "4. **Apilamiento de frames**\n",
    "   Finalmente, acumulamos las √∫ltimas 4 im√°genes preprocesadas (NUM_STACKED_FRAMES = 4) en un √∫nico tensor de entrada. As√≠ el agente puede inferir la velocidad y direcci√≥n de los elementos m√≥viles a partir de la diferencia entre frames.\n",
    "\n",
    "Este esquema de preprocesado es fundamental para disminuir el espacio de entrada, acelerar las convoluciones y proporcionar al Q-net una representaci√≥n compacta y rica en informaci√≥n din√°mica, tal como se describe en el algoritmo 1 del paper original .\n",
    "\n",
    "> Se recomienda ver el m√©todo `make_env` en el archivo `utils.py` para entender c√≥mo se implementa este preprocesado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAY_SCALE = True # si True, convertimos la imagen a escala de grises\n",
    "SCREEN_SIZE = 84 # redimensionamos a SCREEN_SIZExSCREEN_SIZE\n",
    "NUM_STACKED_FRAMES = 4 # apilamos NUM_STACKED_FRAMES frames\n",
    "SKIP_FRAMES = 4 # saltamos SKIP_FRAMES frames (haciendo la misma acci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loVxQPrwt9I5",
    "outputId": "18b7ed97-88dd-4b1e-a2cc-b2636686bfc7"
   },
   "outputs": [],
   "source": [
    "# https://ale.farama.org/environments/breakout/\n",
    "ENV_NAME = \"ALE/Breakout-v5\" \n",
    "\n",
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/random',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=1,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "\n",
    "for episode_num in range(1):\n",
    "    obs, info = env.reset()\n",
    "    show_observation_stack(obs)\n",
    "    reward_total = 0\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # seleccionamos una acci√≥n aleatoria\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        reward_total += reward\n",
    "        episode_over = terminated or truncated\n",
    "    print(f\"Episode {episode_num + 1} finished with total reward: {reward_total}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem√°s, podemos mostrar los videos capturados por el entorno de Atari de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo de v√≠deo en tu sistema de ficheros\n",
    "video_path = \"./videos/random/breakout-episode-0.mp4\"\n",
    "\n",
    "# Muestra el v√≠deo\n",
    "Video(video_path, embed=True, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bYVG_TKt9I5"
   },
   "source": [
    "# Exploraci√≥n del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actions shape:\",env.action_space)\n",
    "print(\"Observation shape:\",env.observation_space.shape)\n",
    "env.reset()\n",
    "next_state, reward, terminated, truncated, info = env.step(action=0)\n",
    "\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {terminated},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIHpwiaat9I7"
   },
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "Deep Q Learning extiende el algoritmo cl√°sico de Q-learning al emplear una **red neuronal profunda** como aproximador de la funci√≥n de valor $Q(s,a)$. Inspirado en Mnih et al. (2013), este m√©todo utiliza una **red convolucional** para procesar directamente las im√°genes del entorno Atari, un **replay buffer** para romper la correlaci√≥n temporal de las muestras. La pol√≠tica sigue un esquema **Œµ-greedy**, balanceando exploraci√≥n y explotaci√≥n, y se entrena minimizando el error de la ecuaci√≥n de Bellman sobre lotes de transiciones muestreadas de manera aleatoria.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.researchgate.net/profile/Faris-Mismar/publication/327045314/figure/fig4/AS:819677282455553@1572437701142/Structure-of-the-neural-network-used-for-the-Deep-Q-learning-Network-implementation-with.png\" alt=\"DQN\"/>\n",
    "</p>\n",
    "\n",
    "Fuente: [arXiv:1312.5602](https://arxiv.org/abs/1312.5602)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajTGajUftSgY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Memoria\n",
    "\n",
    "El algoritmo de Deep Q Learning presentado en el paper utiliza una memoria (llamada Replay Memory) para almacenar transiciones pasadas. Tuplas que contienen un estado base, la accion tomada, la recompensa obtenida, una bandera que indica si el siguiente estado es final o no; y el estado siguiente.\n",
    "\n",
    "Esta memoria es circular, es decir, tiene un l√≠mite maximo de elementos y una vez est√© llena comienza a reemplazar los elementos m√°s viejos.\n",
    "\n",
    "Vamos a necesitar crear una funci√≥n **sample** que obtiene una mustra aleatoria de elementos de la memoria.  Esto puede ser una lista de Transiciones o listas separadas (pero alineadas) de los elementos que las componen.\n",
    "\n",
    "\n",
    "> Para implementar esta funcionalidad se debe modificar el archivo **replay_memory.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scPtpbz4tTAh",
    "outputId": "0890f48a-e673-4416-bd69-7dcf2730ba64",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from replay_memory import ReplayMemory, Transition\n",
    "\n",
    "# Creamos la memoria de repetici√≥n\n",
    "replay_memory = ReplayMemory(3)\n",
    "\n",
    "# A√±adimos transiciones a la memoria (solo las 3 √∫ltimas se guardan)\n",
    "replay_memory.add('state1', 'action1', 'reward1', 'done1', 'next_state1' )\n",
    "replay_memory.add('state2', 'action2', 'reward2', 'done2', 'next_state2')\n",
    "replay_memory.add('state3', 'action3', 'reward3', 'done3', 'next_state3')\n",
    "replay_memory.add('state4', 'action4', 'reward4', 'done4', 'next_state4')\n",
    "\n",
    "# Mostramos el tama√±o de la memoria\n",
    "print(f\"Memory size: {len(replay_memory)}\\n\")\n",
    "\n",
    "# Mostramos un sample de la memoria\n",
    "sampled = replay_memory.sample(2)\n",
    "print(f\"Memory sample:\")\n",
    "for i, sample in enumerate(sampled):\n",
    "    print(f\"Sample {i}: {sample}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7Ygv5Mjtb-F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Vamos a usar un mismo modelo FeedForward para estos dos problemas (entrenado en cada problema particular). Recomendamos simplicidad en la creaci√≥n del mismo, pero tienen total libertad al momento de implementarlo.\n",
    "\n",
    "> Para implementar esta funcionalidad se debe modificar el archivo **dqn_cnn_model.py**. Se recomienda empezar por una arquitectura simple como la que se muestra en el paper de Mnih et al. (2013) y luego experimentar con arquitecturas m√°s complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkNBvJB6ryp7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dqn_cnn_model import DQN_CNN_Model\n",
    "\n",
    "env = make_env(ENV_NAME,\n",
    "                record_every=None,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "print(\"Actions shape:\",env.action_space)\n",
    "print(\"Observatiion shape:\",env.observation_space.shape)\n",
    "\n",
    "env.close()\n",
    "\n",
    "cnn_model = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "summary(cnn_model, input_size=(32, SKIP_FRAMES, SCREEN_SIZE, SCREEN_SIZE), device=DEVICE) # 32 es el batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red esta definida para que reciba un tensor de 4 dimensiones (batch_size, num_frames, height, width) y devuelve un tensor de 2 dimensiones (batch_size, num_actions). La funci√≥n `forward` es la encargada de definir el flujo de datos a trav√©s de la red. En este caso, se utiliza una red convolucional seguida de capas totalmente conectadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor_batch = torch.rand((NUM_STACKED_FRAMES,SCREEN_SIZE,SCREEN_SIZE)).unsqueeze(0).to(DEVICE) # A√±adimos una dimensi√≥n para el batch y lo pasamos al dispositivo\n",
    "print(f\"Q-values shape: {cnn_model(obs_tensor_batch).shape}\") # shape: (1, num_actions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de Q se obtienen a partir de la salida de la red, cada columna representa el valor Q para cada acci√≥n posible en el estado actual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model(obs_tensor_batch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos preguntar cu√°l es la acci√≥n con mayor valor Q en un estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model(obs_tensor_batch).max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejemplo vamos a ver c√≥mo tomar los valores de acciones deseables para un conjunto de estados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos 3 observaciones aleatorias para probar el modelo\n",
    "obs_ran1 = torch.rand(4, 84, 84)\n",
    "obs_ran2 = torch.rand(4, 84, 84)\n",
    "obs_ran3 = torch.rand(4, 84, 84)\n",
    "\n",
    "batch = torch.stack([obs_ran1, obs_ran2, obs_ran3], dim=0).to(DEVICE) # shape: (3, 4, 84, 84)\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "\n",
    "actions =  torch.tensor([1, 2, 3], device=DEVICE).unsqueeze(1) # queremos la acci√≥n 1 para la primera observaci√≥n, la acci√≥n 2 para la segunda y la acci√≥n 3 para la tercera\n",
    "\n",
    "Q_test = cnn_model(batch)\n",
    "print(f\"Q-values: {Q_test}\")\n",
    "print(f\"Q-values: {Q_test.gather(1, actions)}\") # https://pytorch.org/docs/main/generated/torch.gather.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi \n",
    "\n",
    "La funci√≥n para procesar los estados (phi en el paper) que es necesaria para poder usar el modelo de Pytorch con las representaciones de gym. Esta funci√≥n pasa una observaci√≥n de gym a un tensor de Pytorch y la normaliza.\n",
    "\n",
    "> T√©cnicamente la funci√≥n phi tiene m√°s responsabilidades, como la de apilar los frames y el downsampling. En nuestro caso se lo delegamos a los wrappers de gymnasium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oegpMg25t9I9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_state(obs):\n",
    "    \"\"\"\n",
    "    Preprocess the state to be used as input for the model (transform to tensor).\n",
    "    \"\"\"\n",
    "    return torch.tensor(obs, dtype=torch.float32, device=DEVICE) / 255.0\n",
    "\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = process_state(obs)\n",
    "print(f\"Observation shape: {obs_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9B7ZY9Htj_F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Agente\n",
    "\n",
    "Vamos a definir una clase agente (abstracto), encargado de interactuar con el ambiente y entrenar los modelos. Los m√©tdos definidos deben funcionar para ambos problemas simplemente cambiando el modelo a utilizar para cada ambiente.\n",
    "\n",
    "Abajo dejamos un esqueleto del mismo y las funciones a completar. Recomendamos no alterar la estructura del mismo, pero pueden definir las funciones auxiliares que consideren necesarias.\n",
    "\n",
    "> Para implementar esta funcionalidad se debe modificar los archivos **abstract_agent.py** y **dqn_agent.py**.\n",
    "\n",
    "Funciones a completar:\n",
    "\n",
    "\n",
    "1. **init**: que inicializa los parametros del agente.\n",
    "\n",
    "2. **compute_epsilon**: que computa el valor actual de epsilon en base al n√∫mero de pasos actuales y si esta entrenando o no.\n",
    "\n",
    "3. **select_action**: Seleccionando acciones \"epsilongreedy-mente\" si estamos entranando y completamente greedy en otro caso.\n",
    "\n",
    "4. **train**: que entrena el agente por un n√∫mero dado de episodios de largo determinado.\n",
    "\n",
    "5. **record_test_episode**: para grabar un episodio con el agente siempre seleccionando la mejor accion conocida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOD-ENZRtyMt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperpar√°metros de entrenamiento del agente DQN\n",
    "# TOTAL_STEPS = 10_000_000\n",
    "TOTAL_STEPS = 10_000\n",
    "EPISODES = 10_000\n",
    "STEPS_PER_EPISODE = 20_000\n",
    "\n",
    "EPSILON_INI = 1\n",
    "EPSILON_MIN = 0.05\n",
    "# EPSILON_ANNEAL_STEPS = 1_000_000\n",
    "EPSILON_ANNEAL_STEPS = 10_000\n",
    "\n",
    "EPISODE_BLOCK = 100\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50_000\n",
    "\n",
    "GAMMA = 0.995\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/dqn_training',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=500,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import replay_memory  # or whatever module you changed\n",
    "import dqn_cnn_model\n",
    "import dqn_agent\n",
    "import abstract_agent\n",
    "importlib.reload(replay_memory)\n",
    "importlib.reload(dqn_cnn_model)\n",
    "importlib.reload(dqn_agent)\n",
    "importlib.reload(abstract_agent)\n",
    "from dqn_cnn_model import DQN_CNN_Model\n",
    "from dqn_agent import DQNAgent\n",
    "\n",
    "from abstract_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(env, net, process_state, BUFFER_SIZE, BATCH_SIZE, LEARNING_RATE, GAMMA, epsilon_i=EPSILON_INI, epsilon_f=EPSILON_MIN, epsilon_anneal_steps=EPSILON_ANNEAL_STEPS, episode_block=EPISODE_BLOCK, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BsTl-pFqt10b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   0%|          | 2/10000 [00:04<5:53:39,  2.12s/episode, reward=3.5, epsilon=0.941, steps=618]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dqn_rewards = \u001b[43mdqn_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPISODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS_PER_EPISODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOTAL_STEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Octav\\postgrado\\taller ia\\obligatorio\\git\\obligatorio-taller-ia\\abstract_agent.py:63\u001b[39m, in \u001b[36mAgent.train\u001b[39m\u001b[34m(self, number_episodes, max_steps_episode, max_steps)\u001b[39m\n\u001b[32m     60\u001b[39m action = \u001b[38;5;28mself\u001b[39m.select_action(state_phi, total_steps, train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Ejecutar action = env.step(action)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m next_state, reward, done, _, _, = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Procesar next_state con state_processing_function\u001b[39;00m\n\u001b[32m     66\u001b[39m next_state_phi = \u001b[38;5;28mself\u001b[39m.state_processing_function(next_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Octav\\anaconda3\\envs\\obl_taller_ia\\Lib\\site-packages\\gymnasium\\core.py:585\u001b[39m, in \u001b[36mRewardWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    582\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    583\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    584\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m.reward(reward), terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Octav\\anaconda3\\envs\\obl_taller_ia\\Lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:416\u001b[39m, in \u001b[36mFrameStackObservation.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    407\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    408\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[32m    409\u001b[39m \n\u001b[32m    410\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m \u001b[33;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mself\u001b[39m.obs_queue.append(obs)\n\u001b[32m    419\u001b[39m     updated_obs = deepcopy(\n\u001b[32m    420\u001b[39m         concatenate(\u001b[38;5;28mself\u001b[39m.env.observation_space, \u001b[38;5;28mself\u001b[39m.obs_queue, \u001b[38;5;28mself\u001b[39m.stacked_obs)\n\u001b[32m    421\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Octav\\anaconda3\\envs\\obl_taller_ia\\Lib\\site-packages\\gymnasium\\wrappers\\atari_preprocessing.py:184\u001b[39m, in \u001b[36mAtariPreprocessing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    182\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m             \u001b[38;5;28mself\u001b[39m.ale.getScreenRGB(\u001b[38;5;28mself\u001b[39m.obs_buffer[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, total_reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Octav\\anaconda3\\envs\\obl_taller_ia\\Lib\\site-packages\\gymnasium\\wrappers\\atari_preprocessing.py:219\u001b[39m, in \u001b[36mAtariPreprocessing._get_obs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m     np.maximum(\u001b[38;5;28mself\u001b[39m.obs_buffer[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.obs_buffer[\u001b[32m1\u001b[39m], out=\u001b[38;5;28mself\u001b[39m.obs_buffer[\u001b[32m0\u001b[39m])\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m obs = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobs_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscreen_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mINTER_AREA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scale_obs:\n\u001b[32m    226\u001b[39m     obs = np.asarray(obs, dtype=np.float32) / \u001b[32m255.0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dqn_rewards = dqn_agent.train(EPISODES, STEPS_PER_EPISODE, TOTAL_STEPS)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/dqn_validation',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=1,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.play(env, episodes=3)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo de v√≠deo en tu sistema de ficheros\n",
    "video_path = \"./videos/dqn_validation/breakout-episode-0.mp4\"\n",
    "\n",
    "# Muestra el v√≠deo\n",
    "Video(video_path, embed=True, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficas \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QETh1K7pt9I_"
   },
   "source": [
    "# Double Deep Q Learning\n",
    "\n",
    "Double DQN mejora la versi√≥n cl√°sica de DQN corrigiendo el sesgo de sobreestimaci√≥n de los valores \n",
    "ùëÑ\n",
    "Q. Para ello, desacopla la selecci√≥n de la acci√≥n de su evaluaci√≥n: en cada paso, la red online elige la acci√≥n que maximiza \n",
    "ùëÑ\n",
    "Q, pero la red objetivo distinta estima el valor de esa acci√≥n. Esta separaci√≥n reduce el sesgo hacia valores demasiado optimistas y aporta mayor estabilidad al entrenamiento. El resto de la estructura ‚Äîreplay buffer, pol√≠tica Œµ-greedy, etc‚Äî se mantiene igual que en DQN, aprovechando as√≠ un dise√±o casi id√©ntico al original pero con resultados m√°s fiables .\n",
    "\n",
    "Fuente: [arXiv:1509.06461](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "> Vamos a utilizar el mismo modelo de red neuronal creado para el problema anterior y la misma implementaci√≥n de memoria, dejamos un esqueleto de un agente de Double Deep Q learning para completar en el archivo **double_dqn_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDNkAtdMt9I_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from double_dqn_agent import DoubleDQNAgent\n",
    "\n",
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/ddqn_training',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=500,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "\n",
    "\n",
    "modelo_a = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "modelo_b = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "\n",
    "ddqn_agent = DoubleDQNAgent(env, modelo_a, modelo_b, process_state, BUFFER_SIZE, BATCH_SIZE, LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI, epsilon_f=EPSILON_MIN, epsilon_anneal_steps=EPSILON_ANNEAL_STEPS, episode_block = EPISODE_BLOCK, device=DEVICE)\n",
    "\n",
    "ddqn_rewards = ddqn_agent.train(EPISODES, STEPS_PER_EPISODE, TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/ddqn_validation',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=1,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "\n",
    "ddqn_agent.play(env, episodes=3)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo de v√≠deo en tu sistema de ficheros\n",
    "video_path = \"./videos/ddqn_validation/breakout-episode-0.mp4\"\n",
    "\n",
    "# Muestra el v√≠deo\n",
    "Video(video_path, embed=True, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficas \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentaci√≥n\n",
    "Aqu√≠ con libertad total, pueden probar diferentes arquitecturas de red, diferentes hiperpar√°metros, diferentes t√©cnicas de exploraci√≥n, etc.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNNvsKiEt9I_"
   },
   "source": [
    "# Comparaciones, Resultados, Comentarios...\n",
    "De aqu√≠ en adelante son libres de presentar como gusten los resultados comparativos de las t√©cnicas.\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "obl_taller_ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "654d30e281ea4991f82051f147fbd22a65ce2ea742e27edf10b646598953974d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
